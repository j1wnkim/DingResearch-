{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 09:51:44.876787: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-04 09:51:44.952931: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-04 09:51:45.181683: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-04 09:51:45.182659: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-04 09:51:46.204541: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "from tensorflow import keras \n",
    "import tensorflow \n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0+cu117'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Data Setup "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining methods to set up data to return sequences of varying lengths (n_steps) for x and y:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also plan to try to clean up the data more exhaustively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Demand</th>\n",
       "      <th>Annual Usage (kWh)</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>WeekDay or Weekend</th>\n",
       "      <th>HVN_lon</th>\n",
       "      <th>HVN_lat</th>\n",
       "      <th>HVN_tmpf</th>\n",
       "      <th>...</th>\n",
       "      <th>DXR_drct</th>\n",
       "      <th>DXR_sknt</th>\n",
       "      <th>DXR_vsby</th>\n",
       "      <th>MMK_lon</th>\n",
       "      <th>MMK_lat</th>\n",
       "      <th>MMK_tmpf</th>\n",
       "      <th>MMK_drct</th>\n",
       "      <th>MMK_sknt</th>\n",
       "      <th>MMK_vsby</th>\n",
       "      <th>HotTemperatures</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3053.00</td>\n",
       "      <td>13244.454545</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-72.8868</td>\n",
       "      <td>41.2638</td>\n",
       "      <td>33.980000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>-72.83</td>\n",
       "      <td>41.51</td>\n",
       "      <td>28.94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2892.00</td>\n",
       "      <td>13244.454545</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-72.8868</td>\n",
       "      <td>41.2638</td>\n",
       "      <td>37.940000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>-72.83</td>\n",
       "      <td>41.51</td>\n",
       "      <td>28.94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2774.00</td>\n",
       "      <td>13244.454545</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-72.8868</td>\n",
       "      <td>41.2638</td>\n",
       "      <td>37.040000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>-72.83</td>\n",
       "      <td>41.51</td>\n",
       "      <td>33.08</td>\n",
       "      <td>150.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2710.00</td>\n",
       "      <td>13244.454545</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-72.8868</td>\n",
       "      <td>41.2638</td>\n",
       "      <td>33.980000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>-72.83</td>\n",
       "      <td>41.51</td>\n",
       "      <td>33.08</td>\n",
       "      <td>150.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2698.00</td>\n",
       "      <td>13244.454545</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>-72.8868</td>\n",
       "      <td>41.2638</td>\n",
       "      <td>30.920000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>-72.83</td>\n",
       "      <td>41.51</td>\n",
       "      <td>28.94</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96422</th>\n",
       "      <td>3310.60</td>\n",
       "      <td>14064.500000</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>-72.8868</td>\n",
       "      <td>41.2638</td>\n",
       "      <td>46.133333</td>\n",
       "      <td>...</td>\n",
       "      <td>46.666667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>-72.83</td>\n",
       "      <td>41.51</td>\n",
       "      <td>43.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96423</th>\n",
       "      <td>3148.00</td>\n",
       "      <td>14064.500000</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>-72.8868</td>\n",
       "      <td>41.2638</td>\n",
       "      <td>46.133333</td>\n",
       "      <td>...</td>\n",
       "      <td>46.666667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>-72.83</td>\n",
       "      <td>41.51</td>\n",
       "      <td>43.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96424</th>\n",
       "      <td>2988.93</td>\n",
       "      <td>14064.500000</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>-72.8868</td>\n",
       "      <td>41.2638</td>\n",
       "      <td>46.133333</td>\n",
       "      <td>...</td>\n",
       "      <td>46.666667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>-72.83</td>\n",
       "      <td>41.51</td>\n",
       "      <td>43.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96425</th>\n",
       "      <td>2793.70</td>\n",
       "      <td>14064.500000</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>-72.8868</td>\n",
       "      <td>41.2638</td>\n",
       "      <td>46.133333</td>\n",
       "      <td>...</td>\n",
       "      <td>46.666667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>-72.83</td>\n",
       "      <td>41.51</td>\n",
       "      <td>43.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96426</th>\n",
       "      <td>2629.35</td>\n",
       "      <td>14064.500000</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>-72.8868</td>\n",
       "      <td>41.2638</td>\n",
       "      <td>46.133333</td>\n",
       "      <td>...</td>\n",
       "      <td>46.666667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>-72.83</td>\n",
       "      <td>41.51</td>\n",
       "      <td>43.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96427 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Demand  Annual Usage (kWh)  Year  Month  Day  Hour  \\\n",
       "0      3053.00        13244.454545  2011      1    1     0   \n",
       "1      2892.00        13244.454545  2011      1    1     1   \n",
       "2      2774.00        13244.454545  2011      1    1     2   \n",
       "3      2710.00        13244.454545  2011      1    1     3   \n",
       "4      2698.00        13244.454545  2011      1    1     4   \n",
       "...        ...                 ...   ...    ...  ...   ...   \n",
       "96422  3310.60        14064.500000  2021     12   31    19   \n",
       "96423  3148.00        14064.500000  2021     12   31    20   \n",
       "96424  2988.93        14064.500000  2021     12   31    21   \n",
       "96425  2793.70        14064.500000  2021     12   31    22   \n",
       "96426  2629.35        14064.500000  2021     12   31    23   \n",
       "\n",
       "       WeekDay or Weekend  HVN_lon  HVN_lat   HVN_tmpf  ...   DXR_drct  \\\n",
       "0                       0 -72.8868  41.2638  33.980000  ...   0.000000   \n",
       "1                       0 -72.8868  41.2638  37.940000  ...   0.000000   \n",
       "2                       0 -72.8868  41.2638  37.040000  ...   0.000000   \n",
       "3                       0 -72.8868  41.2638  33.980000  ...   0.000000   \n",
       "4                       0 -72.8868  41.2638  30.920000  ...   0.000000   \n",
       "...                   ...      ...      ...        ...  ...        ...   \n",
       "96422                   1 -72.8868  41.2638  46.133333  ...  46.666667   \n",
       "96423                   1 -72.8868  41.2638  46.133333  ...  46.666667   \n",
       "96424                   1 -72.8868  41.2638  46.133333  ...  46.666667   \n",
       "96425                   1 -72.8868  41.2638  46.133333  ...  46.666667   \n",
       "96426                   1 -72.8868  41.2638  46.133333  ...  46.666667   \n",
       "\n",
       "       DXR_sknt  DXR_vsby  MMK_lon  MMK_lat  MMK_tmpf  MMK_drct  MMK_sknt  \\\n",
       "0      0.000000  5.000000   -72.83    41.51     28.94       0.0       0.0   \n",
       "1      0.000000  5.000000   -72.83    41.51     28.94       0.0       0.0   \n",
       "2      0.000000  4.000000   -72.83    41.51     33.08     150.0       5.0   \n",
       "3      0.000000  4.000000   -72.83    41.51     33.08     150.0       4.0   \n",
       "4      0.000000  4.000000   -72.83    41.51     28.94     100.0       3.0   \n",
       "...         ...       ...      ...      ...       ...       ...       ...   \n",
       "96422  1.333333  6.666667   -72.83    41.51     43.00       0.0       0.0   \n",
       "96423  1.333333  6.666667   -72.83    41.51     43.00       0.0       0.0   \n",
       "96424  1.333333  6.666667   -72.83    41.51     43.00       0.0       0.0   \n",
       "96425  1.333333  6.666667   -72.83    41.51     43.00       0.0       0.0   \n",
       "96426  1.333333  6.666667   -72.83    41.51     43.00       0.0       0.0   \n",
       "\n",
       "       MMK_vsby  HotTemperatures  \n",
       "0           4.0                0  \n",
       "1           4.0                0  \n",
       "2           5.0                0  \n",
       "3           6.0                0  \n",
       "4           5.0                0  \n",
       "...         ...              ...  \n",
       "96422       4.0                0  \n",
       "96423       4.0                0  \n",
       "96424       4.0                0  \n",
       "96425       4.0                0  \n",
       "96426       4.0                0  \n",
       "\n",
       "[96427 rows x 56 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TransformerData = pd.read_csv(\"/home/jik19004/FilesToRun/DINGRESEARCH/DingResearchDec28th/DingLSTMFolder/FINALIZED_DATA.csv\")\n",
    "display(TransformerData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Annual Usage (kWh)</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>WeekDay or Weekend</th>\n",
       "      <th>HVN_lon</th>\n",
       "      <th>HVN_lat</th>\n",
       "      <th>HVN_tmpf</th>\n",
       "      <th>HVN_drct</th>\n",
       "      <th>...</th>\n",
       "      <th>DXR_drct</th>\n",
       "      <th>DXR_sknt</th>\n",
       "      <th>DXR_vsby</th>\n",
       "      <th>MMK_lon</th>\n",
       "      <th>MMK_lat</th>\n",
       "      <th>MMK_tmpf</th>\n",
       "      <th>MMK_drct</th>\n",
       "      <th>MMK_sknt</th>\n",
       "      <th>MMK_vsby</th>\n",
       "      <th>HotTemperatures</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13244.454545</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-72.8868</td>\n",
       "      <td>41.2638</td>\n",
       "      <td>33.980000</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>-72.83</td>\n",
       "      <td>41.51</td>\n",
       "      <td>28.94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13244.454545</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-72.8868</td>\n",
       "      <td>41.2638</td>\n",
       "      <td>37.940000</td>\n",
       "      <td>63.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>-72.83</td>\n",
       "      <td>41.51</td>\n",
       "      <td>28.94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13244.454545</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-72.8868</td>\n",
       "      <td>41.2638</td>\n",
       "      <td>37.040000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>-72.83</td>\n",
       "      <td>41.51</td>\n",
       "      <td>33.08</td>\n",
       "      <td>150.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13244.454545</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-72.8868</td>\n",
       "      <td>41.2638</td>\n",
       "      <td>33.980000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>-72.83</td>\n",
       "      <td>41.51</td>\n",
       "      <td>33.08</td>\n",
       "      <td>150.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13244.454545</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>-72.8868</td>\n",
       "      <td>41.2638</td>\n",
       "      <td>30.920000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>-72.83</td>\n",
       "      <td>41.51</td>\n",
       "      <td>28.94</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96422</th>\n",
       "      <td>14064.500000</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>-72.8868</td>\n",
       "      <td>41.2638</td>\n",
       "      <td>46.133333</td>\n",
       "      <td>86.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>46.666667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>-72.83</td>\n",
       "      <td>41.51</td>\n",
       "      <td>43.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96423</th>\n",
       "      <td>14064.500000</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>-72.8868</td>\n",
       "      <td>41.2638</td>\n",
       "      <td>46.133333</td>\n",
       "      <td>86.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>46.666667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>-72.83</td>\n",
       "      <td>41.51</td>\n",
       "      <td>43.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96424</th>\n",
       "      <td>14064.500000</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>-72.8868</td>\n",
       "      <td>41.2638</td>\n",
       "      <td>46.133333</td>\n",
       "      <td>86.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>46.666667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>-72.83</td>\n",
       "      <td>41.51</td>\n",
       "      <td>43.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96425</th>\n",
       "      <td>14064.500000</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>-72.8868</td>\n",
       "      <td>41.2638</td>\n",
       "      <td>46.133333</td>\n",
       "      <td>86.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>46.666667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>-72.83</td>\n",
       "      <td>41.51</td>\n",
       "      <td>43.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96426</th>\n",
       "      <td>14064.500000</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>-72.8868</td>\n",
       "      <td>41.2638</td>\n",
       "      <td>46.133333</td>\n",
       "      <td>86.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>46.666667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>-72.83</td>\n",
       "      <td>41.51</td>\n",
       "      <td>43.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96427 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Annual Usage (kWh)  Year  Month  Day  Hour  WeekDay or Weekend  \\\n",
       "0            13244.454545  2011      1    1     0                   0   \n",
       "1            13244.454545  2011      1    1     1                   0   \n",
       "2            13244.454545  2011      1    1     2                   0   \n",
       "3            13244.454545  2011      1    1     3                   0   \n",
       "4            13244.454545  2011      1    1     4                   0   \n",
       "...                   ...   ...    ...  ...   ...                 ...   \n",
       "96422        14064.500000  2021     12   31    19                   1   \n",
       "96423        14064.500000  2021     12   31    20                   1   \n",
       "96424        14064.500000  2021     12   31    21                   1   \n",
       "96425        14064.500000  2021     12   31    22                   1   \n",
       "96426        14064.500000  2021     12   31    23                   1   \n",
       "\n",
       "       HVN_lon  HVN_lat   HVN_tmpf    HVN_drct  ...   DXR_drct  DXR_sknt  \\\n",
       "0     -72.8868  41.2638  33.980000  190.000000  ...   0.000000  0.000000   \n",
       "1     -72.8868  41.2638  37.940000   63.333333  ...   0.000000  0.000000   \n",
       "2     -72.8868  41.2638  37.040000  200.000000  ...   0.000000  0.000000   \n",
       "3     -72.8868  41.2638  33.980000  130.000000  ...   0.000000  0.000000   \n",
       "4     -72.8868  41.2638  30.920000  130.000000  ...   0.000000  0.000000   \n",
       "...        ...      ...        ...         ...  ...        ...       ...   \n",
       "96422 -72.8868  41.2638  46.133333   86.666667  ...  46.666667  1.333333   \n",
       "96423 -72.8868  41.2638  46.133333   86.666667  ...  46.666667  1.333333   \n",
       "96424 -72.8868  41.2638  46.133333   86.666667  ...  46.666667  1.333333   \n",
       "96425 -72.8868  41.2638  46.133333   86.666667  ...  46.666667  1.333333   \n",
       "96426 -72.8868  41.2638  46.133333   86.666667  ...  46.666667  1.333333   \n",
       "\n",
       "       DXR_vsby  MMK_lon  MMK_lat  MMK_tmpf  MMK_drct  MMK_sknt  MMK_vsby  \\\n",
       "0      5.000000   -72.83    41.51     28.94       0.0       0.0       4.0   \n",
       "1      5.000000   -72.83    41.51     28.94       0.0       0.0       4.0   \n",
       "2      4.000000   -72.83    41.51     33.08     150.0       5.0       5.0   \n",
       "3      4.000000   -72.83    41.51     33.08     150.0       4.0       6.0   \n",
       "4      4.000000   -72.83    41.51     28.94     100.0       3.0       5.0   \n",
       "...         ...      ...      ...       ...       ...       ...       ...   \n",
       "96422  6.666667   -72.83    41.51     43.00       0.0       0.0       4.0   \n",
       "96423  6.666667   -72.83    41.51     43.00       0.0       0.0       4.0   \n",
       "96424  6.666667   -72.83    41.51     43.00       0.0       0.0       4.0   \n",
       "96425  6.666667   -72.83    41.51     43.00       0.0       0.0       4.0   \n",
       "96426  6.666667   -72.83    41.51     43.00       0.0       0.0       4.0   \n",
       "\n",
       "       HotTemperatures  \n",
       "0                    0  \n",
       "1                    0  \n",
       "2                    0  \n",
       "3                    0  \n",
       "4                    0  \n",
       "...                ...  \n",
       "96422                0  \n",
       "96423                0  \n",
       "96424                0  \n",
       "96425                0  \n",
       "96426                0  \n",
       "\n",
       "[96427 rows x 55 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DemandData = TransformerData.loc[:, \"Demand\"].copy()\n",
    "TransformerData.drop(\"Demand\", axis = 1,inplace = True)\n",
    "display(TransformerData)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Our Data so that one particular feature wouldn't dominate the loss in particular! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the Data into Training, Validation, and Testing! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_tuner import RandomSearch\n",
    "from keras_tuner.engine.hyperparameters import HyperParameter \n",
    "import tensorflow as tf \n",
    "from tensorflow import keras as keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_sequences(data, outputData, input_n_steps, output_n_steps):\n",
    "    X = []\n",
    "    Y = []\n",
    "    length = len(data)\n",
    "    for i in range(0,length, output_n_steps):\n",
    "        input_indx = i + input_n_steps\n",
    "        output_indx = input_indx + output_n_steps \n",
    "        if (output_indx > len(data)): # we need to have equally split sequences. \n",
    "            break               # The remaining data that cannot fit into a fixed\n",
    "                                # sequence will immediately be cut!\n",
    "        else:\n",
    "            Xsample = data.iloc[i:input_indx, :] # get the previous data\n",
    "            Ysample = outputData[input_indx:output_indx]\n",
    "            X.append(Xsample)\n",
    "            Y.append(Ysample) \n",
    "    X = np.asarray(X).astype('float64')\n",
    "    Y = np.asarray(Y).astype('float64')\n",
    "    return (X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "def scaleTheData(data):\n",
    "    scaler = StandardScaler()\n",
    "    # split the data first. \n",
    "    data2 = scaler.fit_transform(data)\n",
    "    data = pd.DataFrame(data2, columns = data.columns)\n",
    "    return data\n",
    "\n",
    "def splitDataAndScale(data, output):\n",
    "    \n",
    "    TrainingData = scaleTheData(data.iloc[:61363, :].copy()) # The index for Janruary 1st 2018 basically, don't necessarily hard code it. \n",
    "    TrainingOutput = output[:61363].copy()\n",
    "\n",
    "    RemainingData = data.iloc[61363:, :].copy()\n",
    "    RemainingOutput = output[61363:].copy()\n",
    "\n",
    "    ValidationData = scaleTheData(RemainingData.iloc[:int(0.5*len(RemainingData)), :].copy())\n",
    "    ValidationOutput = RemainingOutput[:int(0.5*len(RemainingOutput))].copy()\n",
    "\n",
    "    TestingData = scaleTheData(RemainingData.iloc[int(0.5 * len(RemainingData)):, :].copy())\n",
    "    TestingOutput = RemainingOutput[int(0.5 * len(RemainingData)):].copy()\n",
    "\n",
    "    TrainingSequences = return_sequences(TrainingData, TrainingOutput, 18, 12)\n",
    "\n",
    "    TransformedTrainingData = TrainingSequences[0]\n",
    "    TransformedTrainingOutput = TrainingSequences[1]\n",
    "\n",
    "    ValidationSequences = return_sequences(ValidationData, ValidationOutput, 18,12)\n",
    "\n",
    "    TransformedValidationData = ValidationSequences[0]\n",
    "    TransformedValidationOutput = ValidationSequences[1]\n",
    "\n",
    "    TestingSequences = return_sequences(TestingData, TestingOutput, 18, 12)\n",
    "\n",
    "    TransformedTestingData = TestingSequences[0]\n",
    "    TransformedTestingOutput = TestingSequences[1]\n",
    "\n",
    "\n",
    "\n",
    "    return (TransformedTrainingData, TransformedTrainingOutput, TransformedValidationData, TransformedValidationOutput, \n",
    "    TransformedTestingData, TransformedTestingOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = splitDataAndScale(TransformerData, DemandData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5112, 18, 55)\n"
     ]
    }
   ],
   "source": [
    "print(data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1459, 18, 55)\n"
     ]
    }
   ],
   "source": [
    "print(data[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1459, 18, 55)\n"
     ]
    }
   ],
   "source": [
    "print(data[4].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData = data[0]\n",
    "trainingOutput = data[1]\n",
    "validationData = data[2]\n",
    "validationOutput = data[3]\n",
    "testingData = data[4]\n",
    "testingnOutput = data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1459\n"
     ]
    }
   ],
   "source": [
    "print(len(data[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingData = [];  \n",
    "for i in range(5112): \n",
    "    TrainingData.append((data[0][i], data[1][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1459, 18, 55)\n",
      "(1459, 12)\n"
     ]
    }
   ],
   "source": [
    "print(validationData.shape)\n",
    "print(validationOutput.shape)\n",
    "\n",
    "ValidationData = []; \n",
    "for i in range(1459):\n",
    "    ValidationData.append((data[2][i], data[3][i]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model Construction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26880/1688313836.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(P)\n"
     ]
    }
   ],
   "source": [
    "def getPositionalEncoding1(batch_size, seq_len, d, n = 10000):\n",
    "        P = np.zeros((batch_size,seq_len, d))\n",
    "        for z in range(batch_size):\n",
    "            for k in range(seq_len):\n",
    "                for i in np.arange(int(d/2)):\n",
    "                    denominator = np.power(n, 2*i/d)\n",
    "                    P[z, k, 2*i] = np.sin(k/denominator)\n",
    "                    P[z,k, 2*i+1] = np.cos(k/denominator)\n",
    "        P = torch.tensor(P).to()\n",
    "        return torch.tensor(P)\n",
    "\n",
    "print(type(getPositionalEncoding1(10, 10, 10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesTransformer(nn.Module): \n",
    "    def __init__(self, num_layers, dropout = 0.4, outfeatures1 = 160, outfeatures2 = 160, dim_feedforward = 2048, output_num = 6):\n",
    "        super().__init__()\n",
    "\n",
    "        self.Linear0 = nn.Linear(in_features = 55, out_features = 110)\n",
    "        self.encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model = 110, nhead = 5, dim_feedforward = dim_feedforward, dropout = dropout, batch_first = True), num_layers = num_layers)\n",
    "        self.batchnorm = nn.BatchNorm1d(num_features = 18)      \n",
    "        self.Flatten = nn.Flatten() # take into account that we're doing this in batches!! \n",
    "        \n",
    "        self.Linear1 = nn.Linear(in_features = 110, out_features = outfeatures1);\n",
    "        self.activation1 = nn.LeakyReLU(negative_slope = 0.25, inplace = True)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(num_features = 18)\n",
    "        self.dropoutLayer1 = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "        # We then slice the last half! \n",
    "        self.Linear2 = nn.Linear(in_features = outfeatures1, out_features = outfeatures2); \n",
    "        self.activation2 = nn.LeakyReLU(negative_slope = 0.25, inplace = True)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(num_features = 18)\n",
    "        self.dropoutLayer2 = nn.Dropout(dropout);\n",
    "\n",
    "        self.Linear3 = nn.Linear(in_features= outfeatures2, out_features = 120)\n",
    "        self.activation3 = nn.LeakyReLU(negative_slope = 0.25, inplace = True)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(num_features = 18)\n",
    "        self.dropoutLayer3 = nn.Dropout(dropout);\n",
    "\n",
    "\n",
    "        self.Linear4 = nn.Linear(in_features= 120, out_features = 180)\n",
    "        self.activation4 = nn.LeakyReLU(negative_slope = 0.25, inplace = True)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(num_features = 18)\n",
    "        self.dropoutLayer4 = nn.Dropout(dropout);\n",
    "\n",
    "\n",
    "        self.Linear5 = nn.Linear(in_features = 180 , out_features = 92) # we will then compute using 2 more fully connected layers. \n",
    "        self.activation5 = nn.ELU(inplace = True);\n",
    "        self.dropoutLayer5 = nn.Dropout(dropout);\n",
    "        self.batchnorm5 = nn.BatchNorm1d(num_features = 18)\n",
    "        \n",
    "\n",
    "        self.Linear6 = nn.Linear(in_features= 92, out_features = 64)\n",
    "        self.activation6 = nn.ELU(inplace = False);\n",
    "        self.dropoutLayer6 = nn.Dropout(dropout);\n",
    "        self.batchnorm6 = nn.BatchNorm1d(num_features = 18)\n",
    "        \n",
    "        self.Linear7 = nn.Linear(in_features = 64, out_features= 16)\n",
    "        self.activation7 = nn.ReLU(inplace= False)\n",
    "\n",
    "\n",
    "        \n",
    "        #self.Linear4 = nn.Linear(in_features = outfeatures3, out_features = 64)\n",
    "        #self.activation4 = nn.LeakyReLU(negative_slope = 0.25, inplace = False)\n",
    "        #self.dropoutLayer4 = nn.Dropout(dropout);\n",
    "\n",
    "\n",
    "        self.Linear8 = nn.Linear(in_features = 16 * 18, out_features = output_num)\n",
    "\n",
    "    def getPositionalEncoding(self, batch_size = 128, seq_len = 18, d = 110, n = 10000, device = torch.device(\"cuda\")):\n",
    "        P = np.zeros((batch_size,seq_len, d))\n",
    "        for z in range(batch_size):\n",
    "            for k in range(seq_len):\n",
    "                for i in np.arange(int(d/2)):\n",
    "                    denominator = np.power(n, 2*i/d)\n",
    "                    P[z, k, 2*i] = np.sin(k/denominator)\n",
    "                    P[z,k, 2*i+1] = np.cos(k/denominator)\n",
    "        P = torch.tensor(P)\n",
    "        P = P.type(\"torch.FloatTensor\")\n",
    "        return P.to(device)\n",
    "\n",
    "    def forward(self, src):\n",
    "        \n",
    "        val = self.Linear0(src)\n",
    "        val = val + self.getPositionalEncoding(batch_size = val.size(0), seq_len = val.size(1), d = val.size(2))\n",
    "        val = self.encoder(val) # this would be (72, 50)\n",
    "        val = self.batchnorm(val)\n",
    "\n",
    "\n",
    "        val = self.Linear1(val) # this would then become (72, 6) \n",
    "        val = self.activation1(val) \n",
    "        val = self.batchnorm1(val) \n",
    "        val = self.dropoutLayer1(val) \n",
    "\n",
    "\n",
    "        val = self.Linear2(val) \n",
    "        val = self.activation2(val)\n",
    "        val = self.batchnorm2(val)\n",
    "        val = self.dropoutLayer2(val)\n",
    "\n",
    "        val = self.Linear3(val)\n",
    "        val = self.activation3(val)\n",
    "        val = self.batchnorm3(val)\n",
    "        val = self.dropoutLayer3(val)\n",
    "                                    # Somehow, we need to conver the tensor of a size [28, 1, 55] into a 2 dimensional vector \n",
    "                                    # where it preserves dimension of the zeroeth index. But have it up to 2nd dimension. \n",
    "                                    # [28, etc] use tensor = torch.randn(x,y,1).squeeze(-1) where x = 10, y = 20, z = 30\n",
    "        val = self.Linear4(val) \n",
    "        val = self.activation4(val)\n",
    "        val = self.batchnorm4(val)\n",
    "        val = self.dropoutLayer4(val) \n",
    "         # convert to 2 dimensional tensor \n",
    "\n",
    "        val = self.Linear5(val)\n",
    "        val = self.activation5(val)\n",
    "        val = self.batchnorm5(val)\n",
    "        val = self.dropoutLayer5(val)\n",
    "        \n",
    "        val = self.Linear6(val)\n",
    "        val = self.activation6(val)\n",
    "        val = self.batchnorm6(val)\n",
    "        val = self.dropoutLayer6(val)\n",
    "        \n",
    "        \n",
    "        \n",
    "        val = self.Linear7(val)\n",
    "        val = self.activation7(val)        \n",
    "        val = val.view(-1, val.size(1) * val.size(2))   \n",
    "\n",
    "\n",
    "        val = self.Linear8(val) # This returns an output of a varying number. \n",
    "        val = val.squeeze()\n",
    "        return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna \n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, output):\n",
    "        data = torch.tensor(data).float(); \n",
    "        output = torch.tensor(output).float() \n",
    "\n",
    "        self.data = data \n",
    "        self.output = output; \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx];\n",
    "        y = self.output[idx];\n",
    "        return x, y;\n",
    "\n",
    "# use the past 72 hours in advance and then predict the 1st hour, 6th hour, 12 hours! \n",
    "\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            running_loss += loss.item() * target.size(0)\n",
    "    return running_loss / len(val_loader.dataset)\n",
    "\n",
    "def Train_and_Evaluate(train_loader, val_loader, device, params, numEpochs, early_stop_epochs):\n",
    "    #num_layers, dropout = 0.1, outfeatures1 = 16, outfeatures2 = 16, outfeatures3 = 16, outfeatures4 = 16, dim_feedforward = 2048, output_num = 6\n",
    "    model = TimeSeriesTransformer(num_layers = params[0], outfeatures1 = params[1], outfeatures2 = params[2],\n",
    "                                dim_feedforward = params[3], output_num = 12)\n",
    "    model = model.to(device);\n",
    "    LossFunction = torch.nn.L1Loss();\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_count = 0\n",
    "    print(\"num layers:\", params[0])\n",
    "    print(\"outfeatures1:\", params[1])\n",
    "    print(\"outfeatures2:\", params[2])\n",
    "    print(\"dim_feedforward:\", params[3])\n",
    "    \n",
    "    Optimizer = torch.optim.Adagrad(params = model.parameters())\n",
    "    for epoch in range(0,numEpochs): \n",
    "        Training_Loss = 0; \n",
    "        total_samples = 0; \n",
    "        for input, output in train_loader:\n",
    "            input = input.to(device); \n",
    "            output = torch.squeeze(output, 1); \n",
    "            output = output.to(device); \n",
    "            predictedVal = model(input)\n",
    "            #predictedVal = torch.squeeze(predictedVal, 1)\n",
    "            Optimizer.zero_grad();\n",
    "            batchLoss = LossFunction(predictedVal, output);\n",
    "            batchLoss.backward(); \n",
    "            Optimizer.step();\n",
    "            Training_Loss += batchLoss * output.size(0) #* output.size(0);\n",
    "            total_samples += output.size(0)\n",
    "        Training_Loss = Training_Loss/total_samples\n",
    "\n",
    "\n",
    "        Validation_Loss = 0; \n",
    "        print(\"passed \", epoch, \"epoch\", \"Training Loss: \", Training_Loss,\" \", end = \"\")\n",
    "        with torch.no_grad():\n",
    "            total_val_samples = 0; \n",
    "            Validation_Loss = 0; \n",
    "            for val_input, val_output in val_loader:\n",
    "                val_input = val_input.to(device); \n",
    "                val_output = torch.squeeze(val_output,1);\n",
    "                val_output = val_output.to(device);\n",
    "\n",
    "                predictedVal = model(val_input)\n",
    "                Validation_Loss += LossFunction(val_output, predictedVal) * val_output.size(0)\n",
    "                total_val_samples += val_output.size(0)\n",
    "            Validation_Loss = Validation_Loss/total_val_samples\n",
    "            print(\"Validation Loss: \", Validation_Loss)\n",
    "\n",
    "            if Validation_Loss < best_val_loss:\n",
    "                best_val_loss = Validation_Loss\n",
    "                early_stop_count = 0;\n",
    "                torch.save(model, \"//home//jik19004//FilesToRun//DINGRESEARCH//DingResearchDec28th//DingTransformerFolderTransformer12\"); #We go save with the transformer!!!\n",
    "            else:\n",
    "                early_stop_count +=1 \n",
    "            if early_stop_count >= early_stop_epochs:\n",
    "                return best_val_loss; \n",
    "            \n",
    "            \n",
    "\n",
    "    return best_val_loss; \n",
    "\n",
    "def predict(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for data, _ in data_loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            predictions.append(output.cpu().numpy())\n",
    "    return np.concatenate(predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation_Loss = 0; \n",
    "        print(\"passed \", epoch, \"epoch\", \"Training Loss: \", Training_Loss,\" \", end = \"\")\n",
    "        with torch.no_grad():\n",
    "            count1 = 0;\n",
    "            for val_input, val_output in val_loader:\n",
    "                val_input = val_input.to(device); \n",
    "                val_output = torch.squeeze(val_output,1);\n",
    "                val_output = val_output.to(device);\n",
    "\n",
    "                predictedVal = model(val_input);\n",
    "                predictedVal = torch.squeeze(predictedVal, 1)\n",
    "                batchLoss = LossFunction(val_output, predictedVal);\n",
    "                Validation_Loss += batchLoss # * val_output.size(0);\n",
    "                count1 += \n",
    "            Validation_Loss = Validation_Loss/count1\n",
    "            print(\"Validation Loss: \", Validation_Loss)\n",
    "\n",
    "            if Validation_Loss < best_val_loss:\n",
    "                best_val_loss = Validation_Loss\n",
    "                early_stop_count = 0;\n",
    "            else:\n",
    "                early_stop_count +=1 \n",
    "            if early_stop_count >= early_stop_epochs:\n",
    "                return best_val_loss; "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GO DEAL WITH NAN VALUES IN OUR DATA!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingData = TimeSeriesDataset(data[0],data[1])\n",
    "ValidationData = TimeSeriesDataset(data[2],data[3])\n",
    "TestingData = TimeSeriesDataset(data[4],data[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingLoader = DataLoader(TrainingData, batch_size = 92);\n",
    "ValidationLoader = DataLoader(ValidationData, batch_size = 128);\n",
    "TestingLoader = DataLoader(TestingData, batch_size = 128);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    #num_layers = params[0], outfeatures = params[1], dim_feedforward = params[2]\n",
    "    params = [trial.suggest_int(\"num_layers\", low = 2, high = 3, step = 2), \n",
    "              trial.suggest_int(\"out_features1\", low = 224, high = 288, step = 32),\n",
    "              trial.suggest_int(\"out_features2\", low = 224, high = 288, step = 32),\n",
    "              trial.suggest_int(\"dim_feedforward\", low = 176, high = 208, step = 8)];\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\");\n",
    "    return Train_and_Evaluate(TrainingLoader, ValidationLoader ,device, params, 260, 10);  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-12-04 09:51:53,129]\u001b[0m Using an existing study with name 'Transformer12' instead of creating a new one.\u001b[0m\n",
      "/home/jik19004/anaconda3/lib/python3.9/site-packages/optuna/distributions.py:716: UserWarning: The distribution is specified by [2, 3] and step=2, but the range is not divisible by `step`. It will be replaced by [2, 2].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num layers: 2\n",
      "outfeatures1: 256\n",
      "outfeatures2: 288\n",
      "dim_feedforward: 176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2023-12-04 09:51:57,766]\u001b[0m Trial 103 failed with parameters: {'num_layers': 2, 'out_features1': 256, 'out_features2': 288, 'dim_feedforward': 176} because of the following error: KeyboardInterrupt().\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jik19004/anaconda3/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_26880/563636019.py\", line 8, in objective\n",
      "    return Train_and_Evaluate(TrainingLoader, ValidationLoader ,device, params, 260, 10);\n",
      "  File \"/tmp/ipykernel_26880/3275244228.py\", line 53, in Train_and_Evaluate\n",
      "    predictedVal = model(input)\n",
      "  File \"/home/jik19004/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_26880/1758547116.py\", line 71, in forward\n",
      "    val = self.Linear0(src)\n",
      "  File \"/home/jik19004/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/jik19004/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "KeyboardInterrupt\n",
      "\u001b[33m[W 2023-12-04 09:51:57,768]\u001b[0m Trial 103 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26880/3841036195.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"minimize\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy_name\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"Transformer12\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_if_exists\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sqlite:///example12.db'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"study12.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m120\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \"\"\"\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    426\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     ):\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_26880/563636019.py\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      6\u001b[0m               trial.suggest_int(\"dim_feedforward\", low = 176, high = 208, step = 8)];\n\u001b[1;32m      7\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTrain_and_Evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainingLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValidationLoader\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m260\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_26880/3275244228.py\u001b[0m in \u001b[0;36mTrain_and_Evaluate\u001b[0;34m(train_loader, val_loader, device, params, numEpochs, early_stop_epochs)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mpredictedVal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0;31m#predictedVal = torch.squeeze(predictedVal, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mOptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_26880/1758547116.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetPositionalEncoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# this would be (72, 50)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import joblib \n",
    "study_name = 'sqlite:///example12.db'\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(), study_name= \"Transformer12\", load_if_exists= True, storage='sqlite:///example12.db')\n",
    "joblib.dump(study, \"study12.pkl\") \n",
    "study.optimize(objective, n_trials = 120) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'dim_feedforward': 184, 'num_layers': 2, 'out_features1': 256, 'out_features2': 288}\n",
      "MAE for validation set: 428.1627502441406\n"
     ]
    }
   ],
   "source": [
    "print(\"Parameters:\", study.best_params)\n",
    "print(\"MAE for validation set:\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d61788d0284415b4fed48a16e8e34bc70a22da2be71b804d332ad1f39f90ff8f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
